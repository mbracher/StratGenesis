# Phase 4: LLM Interface

## Objective

Create an abstraction layer for LLM-driven code generation and mutation. This module interfaces with OpenAI GPT and Anthropic Claude to generate strategy improvements and code.

From the design document:

> This module manages calls to OpenAI and Claude to leverage LLMs for generating strategy mutations. We implement a class `LLMClient` that can interface with either OpenAI's GPT models or Anthropic's Claude.

---

## LLMClient Class

### Constructor

```python
class LLMClient:
    """Interface to LLMs (OpenAI GPT or Anthropic Claude) for generating and modifying strategy code."""

    def __init__(self, provider="openai", model=None, openai_api_key=None, anthropic_api_key=None):
        """
        provider: "openai" or "anthropic"
        model: model name (e.g., "gpt-4" or "claude-2"). If None, uses a default for each provider.
        """
        self.provider = provider

        # Use API keys from environment if not provided
        self.openai_api_key = openai_api_key or os.getenv("OPENAI_API_KEY")
        self.anthropic_api_key = anthropic_api_key or os.getenv("ANTHROPIC_API_KEY")

        # Default models
        if model:
            self.model = model
        else:
            self.model = "gpt-4" if provider == "openai" else "claude-2"

        # Initialize anthropic client if needed
        if provider == "anthropic":
            if anthropic is None:
                raise ImportError("anthropic SDK not installed. Install via `pip install anthropic`")
            self.anthropic_client = anthropic.Client(api_key=self.anthropic_api_key)
        else:
            self.anthropic_client = None

        # Set OpenAI API key
        if provider == "openai":
            openai.api_key = self.openai_api_key
```

---

## Method 1: generate_improvement()

> Prompts an LLM to analyze a strategy's code and recent performance metrics, and then suggest an improvement. This corresponds to LLM A in the ProFiT loop, outputting a natural language proposal Δ (delta) describing how to improve strategy S_t.

### Purpose
- Analyze strategy code and performance
- Identify weaknesses
- Propose specific improvement

### Prompt Template

```python
def generate_improvement(self, strategy_code, metrics_summary):
    """
    Ask the LLM to analyze the strategy code and performance metrics,
    and suggest an improvement (natural language description).
    """
    prompt = (
        "You are an expert trading strategy coach. I will provide you with the code of a trading strategy "
        "and its recent performance metrics. Identify the strategy's weaknesses or areas for improvement, "
        "and propose a specific change or addition to the strategy logic that could improve its performance.\n\n"
        "Strategy Code:\n"
        "```\n" + strategy_code + "\n```\n"
        f"Performance Summary: {metrics_summary}\n\n"
        "Please suggest one concrete improvement (in a brief bullet or sentence)."
    )
    return self._chat(prompt, expect_code=False)
```

### Expected Output
Natural language improvement proposal, e.g.:
- "Increase the short EMA period to respond faster to changes."
- "Add a volatility filter to avoid trading in choppy markets."
- "Implement a trailing stop-loss to protect profits."

---

## Method 2: generate_strategy_code()

> Prompts an LLM to produce modified strategy code given the original code and an improvement proposal. This is LLM B in the loop, responsible for writing the new strategy variant Ŝ_t.

### Purpose
- Apply proposed improvement to base code
- Generate valid `backtesting.Strategy` subclass
- Return only code (no explanations)

### Prompt Template

```python
def generate_strategy_code(self, base_code, improvement_proposal):
    """
    Ask the LLM to apply the proposed improvement to the base strategy code.
    Returns the modified code as a string.
    """
    prompt = (
        "You are a coding assistant specialized in trading strategies. I will provide a base strategy code "
        "and an improvement idea. Your task is to modify the strategy code to implement the improvement. "
        "The code must remain a valid subclass of backtesting.Strategy and retain previous functionality unless changed. "
        "Provide only the full modified code without any explanations.\n\n"
        "Base Strategy Code:\n"
        "```\n" + base_code + "\n```\n"
        "Improvement to implement: " + improvement_proposal + "\n\n"
        "Now output the full updated strategy code (only code, no comments or explanation)."
    )
    return self._chat(prompt, expect_code=True)
```

### Expected Output
Complete Python class code, ready for `exec()`.

---

## Method 3: fix_code()

> If the code generated by LLM B fails to compile or run, we capture the error traceback and call `LLMClient.fix_code()` to prompt the LLM to debug and correct the code. We allow multiple repair attempts (up to 10 as in the paper).

### Purpose
- Diagnose compilation/runtime errors
- Fix code based on traceback
- Enable iterative repair

### Prompt Template

```python
def fix_code(self, base_code, error_trace):
    """
    If the generated code fails, prompt the LLM to fix the error given the traceback.
    """
    prompt = (
        "The following trading strategy code did not compile or run correctly. Error trace:\n"
        f"{error_trace}\n"
        "Please fix the code accordingly. Provide only the corrected code.\n\n"
        "Code with error:\n"
        "```\n" + base_code + "\n```\n"
        "Now output the fixed code."
    )
    return self._chat(prompt, expect_code=True)
```

### Repair Loop (in evolver.py)
```python
for attempt in range(1, 11):  # up to 10 repair attempts
    try:
        exec(new_code, globals(), namespace)
        success = True
        break
    except Exception as e:
        tb = traceback.format_exc()
        if attempt < 10:
            new_code = self.llm.fix_code(new_code, tb)
```

---

## Internal Helper: _chat()

### Purpose
- Unified interface for API calls
- Handle provider-specific formatting
- Strip markdown from code responses

### Implementation

```python
def _chat(self, prompt, expect_code=False):
    """Internal helper to send chat prompt to the chosen provider and return response text."""
    if self.provider == "openai":
        messages = [{"role": "user", "content": prompt}]
        response = openai.ChatCompletion.create(model=self.model, messages=messages)
        text = response['choices'][0]['message']['content']
    elif self.provider == "anthropic":
        full_prompt = f"{anthropic.HUMAN_PROMPT} {prompt} {anthropic.AI_PROMPT}"
        response = self.anthropic_client.completion(
            prompt=full_prompt,
            model=self.model,
            max_tokens_to_sample=1000,
            stop_sequences=[anthropic.HUMAN_PROMPT]
        )
        text = response['completion']
    else:
        raise ValueError("Unsupported provider")

    # If expecting code, strip markdown fences
    if expect_code:
        if text.strip().startswith("```"):
            text = text.strip().strip("```").strip()
            if text.startswith("python"):
                text = text[len("python"):].strip()

    return text
```

---

## Two-LLM Architecture

From the design document:

> By structuring interactions this way, we emulate ProFiT's use of two LLM "roles" – one as an analyst and one as a coder.

```
┌─────────────────┐     Δ (improvement)     ┌─────────────────┐
│     LLM A       │ ───────────────────────▶│     LLM B       │
│   (Analyst)     │                         │    (Coder)      │
│                 │                         │                 │
│ generate_       │                         │ generate_       │
│ improvement()   │                         │ strategy_code() │
└─────────────────┘                         └─────────────────┘
        ▲                                           │
        │                                           │
   Strategy S_t                              Strategy Ŝ_t
   + Metrics                                  (new code)
```

---

## File Structure

```python
# File: llm_interface.py
import openai
import os

try:
    import anthropic
except ImportError:
    anthropic = None

import textwrap

class LLMClient:
    def __init__(self, provider="openai", model=None, openai_api_key=None, anthropic_api_key=None): ...
    def generate_improvement(self, strategy_code, metrics_summary): ...
    def generate_strategy_code(self, base_code, improvement_proposal): ...
    def fix_code(self, base_code, error_trace): ...
    def _chat(self, prompt, expect_code=False): ...
```

---

## API Notes

### OpenAI
- Uses `openai.ChatCompletion.create()`
- System/user message format
- Model examples: "gpt-4", "gpt-3.5-turbo"

### Anthropic
- Uses `anthropic.Client().completion()`
- HUMAN_PROMPT / AI_PROMPT format
- Model examples: "claude-2", "claude-3-opus"
- `max_tokens_to_sample=1000`

---

## Deliverables

- [x] `LLMClient` class with provider abstraction
- [x] `generate_improvement()` method
- [x] `generate_strategy_code()` method
- [x] `fix_code()` method
- [x] `_chat()` internal helper
- [x] Markdown stripping for code responses
- [x] Environment variable support for API keys
