"""Evolutionary engine for strategy optimization.

This module contains the ProfitEvolver class which orchestrates the
evolution loop for trading strategy optimization using LLMs.
"""

from __future__ import annotations

import inspect
import json
import random
import shutil
import traceback
import warnings
from datetime import datetime
from pathlib import Path
from typing import TYPE_CHECKING, Dict, Optional

import numpy as np
import pandas as pd
from backtesting import Backtest, Strategy

from profit.llm_interface import LLMClient
from profit.strategies import (
    BollingerMeanReversion,
    CCIStrategy,
    EMACrossover,
    MACDStrategy,
    WilliamsRStrategy,
    RandomStrategy,
    BuyAndHoldStrategy,
)

if TYPE_CHECKING:
    from profit.program_db import ProgramDatabase


class StrategyPersister:
    """Handles saving evolved strategies to disk."""

    def __init__(self, output_dir: str = "evolved_strategies"):
        """Initialize the persister.

        Args:
            output_dir: Base directory for saving strategies.
        """
        self.output_dir = Path(output_dir)
        self.run_dir: Path | None = None
        self.run_id: str | None = None

    def start_run(
        self,
        seed_strategy_name: str,
        analyst_provider: str,
        analyst_model: str,
        coder_provider: str,
        coder_model: str,
    ) -> Path:
        """Initialize a new run directory.

        Args:
            seed_strategy_name: Name of the seed strategy being evolved.
            analyst_provider: LLM provider for analysis role.
            analyst_model: LLM model for analysis role.
            coder_provider: LLM provider for coding role.
            coder_model: LLM model for coding role.

        Returns:
            Path to the run directory.
        """
        self.run_id = datetime.now().strftime("%Y%m%d_%H%M%S")
        self.run_dir = self.output_dir / f"run_{self.run_id}"
        self.run_dir.mkdir(parents=True, exist_ok=True)

        run_info = {
            "run_id": self.run_id,
            "started_at": datetime.now().isoformat(),
            "seed_strategy": seed_strategy_name,
            "llm_config": {
                "analyst": {
                    "provider": analyst_provider,
                    "model": analyst_model,
                },
                "coder": {
                    "provider": coder_provider,
                    "model": coder_model,
                },
            },
            "folds": [],
        }
        self._write_json(self.run_dir / "run_summary.json", run_info)

        return self.run_dir

    def save_strategy(
        self,
        strategy_class,
        source_code: str,
        fold: int,
        generation: int,
        metrics: dict,
        parent_name: str,
        improvement_proposal: str,
    ) -> Path:
        """Save an evolved strategy to disk.

        Args:
            strategy_class: The strategy class object.
            source_code: Complete Python source code of the strategy.
            fold: Walk-forward fold number (1-indexed).
            generation: Evolution generation number.
            metrics: Performance metrics dict.
            parent_name: Name of the parent strategy.
            improvement_proposal: The LLM's improvement proposal text.

        Returns:
            Path to the saved strategy file.
        """
        fold_dir = self.run_dir / f"fold_{fold}"
        fold_dir.mkdir(exist_ok=True)

        class_name = strategy_class.__name__

        # Save source code as .py file
        strategy_path = fold_dir / f"{class_name}.py"
        header = self._generate_header(class_name, fold, generation, metrics)
        self._write_file(strategy_path, header + source_code)

        # Save metadata as .json file
        metadata = {
            "class_name": class_name,
            "fold": fold,
            "generation": generation,
            "parent_strategy": parent_name,
            "improvement_proposal": improvement_proposal,
            "metrics": metrics,
            "saved_at": datetime.now().isoformat(),
        }
        self._write_json(fold_dir / f"{class_name}.json", metadata)

        return strategy_path

    def _generate_header(
        self, class_name: str, fold: int, generation: int, metrics: dict
    ) -> str:
        """Generate a header comment for the strategy file."""
        ann_return = metrics.get("AnnReturn%")
        sharpe = metrics.get("Sharpe")
        ann_str = f"{ann_return:.2f}%" if ann_return is not None else "N/A"
        sharpe_str = f"{sharpe:.2f}" if sharpe is not None else "N/A"

        return f'''"""Evolved Strategy: {class_name}

Generated by ProFiT evolutionary optimization.
Fold: {fold} | Generation: {generation}
Annualized Return: {ann_str}
Sharpe Ratio: {sharpe_str}

Auto-generated - do not edit manually.
"""

import numpy as np
import pandas as pd
from backtesting import Strategy


'''

    def save_fold_best(
        self, fold: int, strategy_class, source_code: str, metrics: dict
    ) -> Path:
        """Save the best strategy for a fold.

        Args:
            fold: Fold number.
            strategy_class: Best strategy class for this fold.
            source_code: Source code of the strategy.
            metrics: Performance metrics.

        Returns:
            Path to the best strategy file.
        """
        fold_dir = self.run_dir / f"fold_{fold}"
        fold_dir.mkdir(exist_ok=True)
        best_path = fold_dir / "best_strategy.py"

        ann_return = metrics.get("AnnReturn%")
        sharpe = metrics.get("Sharpe")
        ann_str = f"{ann_return:.2f}%" if ann_return is not None else "N/A"
        sharpe_str = f"{sharpe:.2f}" if sharpe is not None else "N/A"

        header = f'''"""Best Strategy for Fold {fold}

Original class: {strategy_class.__name__}
Annualized Return: {ann_str}
Sharpe Ratio: {sharpe_str}
"""

import numpy as np
import pandas as pd
from backtesting import Strategy


'''
        self._write_file(best_path, header + source_code)
        return best_path

    def finalize_run(self, results: list[dict]) -> Path:
        """Finalize the run and save summary.

        Args:
            results: List of per-fold result dictionaries.

        Returns:
            Path to the run summary file.
        """
        summary_path = self.run_dir / "run_summary.json"
        summary = self._read_json(summary_path)

        # Add fold results
        for res in results:
            fold_info = {
                "fold": res["fold"],
                "best_strategy": res["strategy"].__name__,
                "ann_return": res["ann_return"],
                "sharpe": res["sharpe"],
                "expectancy": res["expectancy"],
                "vs_random": res["ann_return"] - res["random_return"],
                "vs_buy_hold": res["ann_return"] - res["buy_hold_return"],
            }
            summary["folds"].append(fold_info)

        # Add aggregate stats
        summary["completed_at"] = datetime.now().isoformat()
        summary["avg_ann_return"] = float(np.mean([r["ann_return"] for r in results]))
        summary["avg_sharpe"] = float(np.mean([r["sharpe"] for r in results]))
        summary["best_fold"] = max(results, key=lambda r: r["ann_return"])["fold"]

        self._write_json(summary_path, summary)

        # Copy best overall strategy
        best_result = max(results, key=lambda r: r["ann_return"])
        best_fold_dir = self.run_dir / f"fold_{best_result['fold']}"
        best_src = best_fold_dir / "best_strategy.py"
        if best_src.exists():
            shutil.copy(best_src, self.run_dir / "best_overall.py")

        return summary_path

    def _write_file(self, path: Path, content: str) -> None:
        """Write string content to file."""
        path.write_text(content)

    def _write_json(self, path: Path, data: dict) -> None:
        """Write dict as JSON to file."""
        path.write_text(json.dumps(data, indent=2))

    def _read_json(self, path: Path) -> dict:
        """Read JSON file as dict."""
        return json.loads(path.read_text())


def load_strategy(strategy_path: str):
    """Load a saved strategy from a .py file.

    Args:
        strategy_path: Path to the saved strategy Python file.

    Returns:
        The strategy class ready for backtesting.

    Example:
        >>> strat = load_strategy("evolved_strategies/run_20250114/fold_1/best_strategy.py")
        >>> bt = Backtest(data, strat, cash=10000)
        >>> bt.run()
    """
    import importlib.util

    path = Path(strategy_path)
    spec = importlib.util.spec_from_file_location(path.stem, path)
    module = importlib.util.module_from_spec(spec)
    spec.loader.exec_module(module)

    # Find the Strategy subclass in the module
    for name, obj in vars(module).items():
        if isinstance(obj, type) and issubclass(obj, Strategy) and obj is not Strategy:
            return obj

    raise ValueError(f"No Strategy subclass found in {strategy_path}")


class ProfitEvolver:
    """Evolutionary search engine for trading strategy optimization.

    Uses LLM-guided code mutation and walk-forward validation to evolve
    strategies that adapt to changing market conditions.

    Phase 14: Supports diff-based mutations for surgical code changes.
    """

    def __init__(
        self,
        llm_client: LLMClient,
        initial_capital: float = 10000,
        commission: float = 0.002,
        exclusive_orders: bool = True,
        output_dir: str | None = None,
        finalize_trades: bool = True,
        program_db: Optional["ProgramDatabase"] = None,
        prefer_diffs: bool = True,
        diff_mode: str = "adaptive",
        diff_match: str = "tolerant",
        exploration_gens: int = 5,
    ):
        """Initialize the ProfitEvolver.

        Args:
            llm_client: An instance of LLMClient for generating strategy mutations.
            initial_capital: Starting cash for backtests (default: $10,000).
            commission: Per-trade commission rate (default: 0.002 = 0.2%).
            exclusive_orders: If True, no overlapping long/short positions.
            output_dir: [DEPRECATED] Directory to save evolved strategies. Use program_db instead.
            finalize_trades: If True, auto-close open trades at backtest end.
            program_db: Optional ProgramDatabase for AlphaEvolve-style storage.
            prefer_diffs: If True, attempt diff-based mutations before full rewrites.
            diff_mode: "always", "never", or "adaptive" (use exploration_gens).
            diff_match: "strict" or "tolerant" for diff matching.
            exploration_gens: In adaptive mode, use full rewrites for first N generations.
        """
        self.llm = llm_client
        self.initial_capital = initial_capital
        self.commission = commission
        self.exclusive_orders = exclusive_orders
        self.finalize_trades = finalize_trades

        # Deprecation warning for output_dir
        if output_dir is not None:
            warnings.warn(
                "output_dir (StrategyPersister) is deprecated. "
                "Use program_db for strategy storage instead. "
                "Use --export-strategy to export strategies from the database.",
                DeprecationWarning,
                stacklevel=2,
            )
        self.persister = StrategyPersister(output_dir) if output_dir else None
        self.program_db = program_db
        # Track DB IDs for strategies: class_name -> db_id mapping
        self._strategy_db_ids: Dict[str, str] = {}

        # Phase 14: Diff-based mutation settings
        self.prefer_diffs = prefer_diffs
        self.diff_mode = diff_mode
        self.diff_match = diff_match
        self.exploration_gens = exploration_gens
        self._consecutive_diff_failures = 0

    def run_backtest(self, strategy_class, data: pd.DataFrame) -> tuple[dict, pd.Series]:
        """Run a backtest on given data with specified strategy class.

        Args:
            strategy_class: A backtesting.Strategy subclass to evaluate.
            data: DataFrame with OHLCV data and datetime index.

        Returns:
            A tuple of (metrics, result) where:
            - metrics: dict with key performance metrics (AnnReturn%, Sharpe, Expectancy%, Trades)
            - result: Full backtesting.py result Series
        """
        bt = Backtest(
            data,
            strategy_class,
            cash=self.initial_capital,
            commission=self.commission,
            exclusive_orders=self.exclusive_orders,
            finalize_trades=self.finalize_trades,
        )
        result = bt.run()

        # Extract key metrics
        metrics = {
            "AnnReturn%": result.get("Return (Ann.) [%]", None),
            "Sharpe": result.get("Sharpe Ratio", None),
            "Expectancy%": result.get("Expectancy [%]", None),
            "Trades": result.get("# Trades", None),
        }
        return metrics, result

    def prepare_folds(
        self, full_data: pd.DataFrame, n_folds: int = 5
    ) -> list[tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]]:
        """Split the full_data DataFrame into train/validation/test folds.

        Implements walk-forward validation with:
        - Training period: 2.5 years (30 months)
        - Validation period: 6 months
        - Test period: 6 months
        - Gap between periods: 10 days (prevents look-ahead bias)

        Args:
            full_data: Complete DataFrame with datetime index and OHLCV columns.
            n_folds: Number of walk-forward folds (default: 5).

        Returns:
            List of (train_df, val_df, test_df) tuples for each fold.
        """
        fold_splits = []
        data_index = full_data.index
        start_date = data_index.min()

        for fold in range(n_folds):
            # Train period: 2.5 years (2 years + 6 months) from current start
            train_end = start_date + pd.DateOffset(years=2) + pd.DateOffset(months=6)

            # Validation start: 10 days after train end (gap)
            val_start = train_end + pd.DateOffset(days=10)
            # Validation end: 6 months after train end
            val_end = train_end + pd.DateOffset(months=6)

            # Test start: 10 days after validation end (gap)
            test_start = val_end + pd.DateOffset(days=10)
            # Test end: 6 months after validation end
            test_end = val_end + pd.DateOffset(months=6)

            # Slice data for each period
            train = full_data[start_date:train_end]
            val = full_data[val_start:val_end]
            test = full_data[test_start:test_end]

            # Stop if we run out of data
            if len(test) == 0:
                break

            fold_splits.append((train, val, test))

            # Move start_date to end of this test period for next fold
            start_date = test_end

        return fold_splits

    def _random_index(self, n: int) -> int:
        """Return a random index in range [0, n).

        Args:
            n: Upper bound (exclusive).

        Returns:
            Random integer in [0, n).
        """
        return random.randrange(n)

    def evolve_strategy(
        self,
        strategy_class,
        train_data: pd.DataFrame,
        val_data: pd.DataFrame,
        max_iters: int = 15,
        fold: int = 1,
    ):
        """Evolve a strategy using LLM-guided mutations.

        Implements the ProFiT evolutionary loop with MAS (Minimum Acceptable Score)
        threshold. New strategies must meet or exceed MAS to be accepted into the
        population.

        Args:
            strategy_class: Seed strategy class to evolve.
            train_data: Training data (for context, not directly used in fitness).
            val_data: Validation data for fitness evaluation.
            max_iters: Maximum number of evolution generations (default: 15).
            fold: Walk-forward fold number for persistence (default: 1).

        Returns:
            Tuple of (best_strategy_class, best_perf, best_code) where:
            - best_strategy_class: The evolved strategy class with highest fitness
            - best_perf: Performance (annualized return %) on validation
            - best_code: Source code of the best strategy
        """
        # 1. Compute baseline performance P0 on validation set
        _, base_result = self.run_backtest(strategy_class, val_data)
        P0 = base_result["Return (Ann.) [%]"]
        print(
            f"Initial strategy {strategy_class.__name__} baseline annualized return "
            f"on validation: {P0:.2f}%"
        )

        # 2. Set MAS = P0 (Minimum Acceptable Score)
        MAS = P0

        # Get source code of the seed strategy
        seed_code = inspect.getsource(strategy_class)

        # Archive of viable strategies (as tuples of class, performance, and source code)
        # We store source code to support dynamically generated strategies
        population = [(strategy_class, P0, seed_code)]
        best_perf = P0
        best_strategy_class = strategy_class
        best_strategy_code = seed_code

        # Register seed strategy in program DB
        if self.program_db:
            from profit.program_db import StrategyStatus

            seed_metrics = self._extract_standard_metrics(base_result)
            seed_id = self.program_db.register_strategy(
                code=seed_code,
                class_name=strategy_class.__name__,
                parent_ids=[],
                mutation_text="Seed strategy",
                metrics=seed_metrics,
                tags=self._infer_tags(seed_code),
                status=StrategyStatus.SEED,
                generation=0,
                fold=fold,
                val_return=P0,
                repair_attempts=0,
            )
            # CRITICAL: Track the DB ID for this class
            self._strategy_db_ids[strategy_class.__name__] = seed_id
            strategy_class._db_id = seed_id
            metrics_str = self._format_metrics_summary(seed_metrics)
            print(f"[{seed_id}] Seed: {strategy_class.__name__} ({metrics_str})")

        # Build exec namespace with necessary imports for generated code
        exec_globals = {
            "Strategy": Strategy,
            "pd": pd,
            "np": np,
        }

        # 4. Evolution loop
        for gen in range(1, max_iters + 1):
            print(
                f"\nGeneration {gen}: Current population size = {len(population)}. "
                "Selecting a strategy to mutate..."
            )

            # 5. Select a strategy from population (random selection for diversity)
            parent_class, parent_perf, parent_code = population[self._random_index(len(population))]
            print(
                f"Selected parent strategy '{parent_class.__name__}' with validation "
                f"return {parent_perf:.2f}% for mutation."
            )

            # 6. Prompt LLM A for improvement proposal
            improvement = self.llm.generate_improvement(
                parent_code, f"AnnReturn={parent_perf:.2f}%"
            )
            print(f"LLM suggested improvement: {improvement}")

            # 7. Prompt LLM B to synthesize modified strategy code
            # Phase 14: Use diff-based approach if enabled
            use_diffs_this_gen = self._should_use_diffs(gen)
            diff_text = None
            mutation_mode = "rewrite"

            if use_diffs_this_gen:
                new_code, used_diff, diff_text = self.llm.generate_strategy_code_with_fallback(
                    parent_code, improvement, match_mode=self.diff_match
                )
                mutation_mode = "diff" if used_diff else "rewrite"
                if used_diff:
                    print(f"Applied diff-based mutation")
                    self._consecutive_diff_failures = 0
                else:
                    print(f"Fell back to full code rewrite")
                    self._consecutive_diff_failures += 1
            else:
                new_code = self.llm.generate_strategy_code(parent_code, improvement)

            # Give the new strategy a unique name by generation
            new_class_name = self._build_strategy_name(parent_class.__name__, gen)

            # Replace class name in code to avoid collisions
            if isinstance(new_code, str) and new_code.startswith("class"):
                # Extract original class name from parent (base name for replacement)
                original_class = parent_class.__name__.split("_")[0]
                if original_class in new_code:
                    new_code = new_code.replace(original_class, new_class_name, 1)
                else:
                    new_code = new_code.replace(parent_class.__name__, new_class_name, 1)

            # 8-11. Try to compile and backtest with repair loop
            success = False
            NewStrategyClass = None
            res = None
            repair_count = 0  # Track number of repair attempts

            for attempt in range(1, 11):  # up to 10 repair attempts
                try:
                    # Dynamically define the new strategy class from code
                    namespace = {}
                    exec(new_code, exec_globals, namespace)
                    NewStrategyClass = namespace[new_class_name]

                    # Run backtest on validation data to get performance
                    _, res = self.run_backtest(NewStrategyClass, val_data)
                    success = True
                    break

                except Exception as e:
                    repair_count = attempt  # Track how many repairs needed
                    tb = traceback.format_exc()
                    print(f"Attempt {attempt}: Strategy code failed with error: {e}")

                    if attempt < 10:
                        # 11. Prompt LLM B with traceback to fix code
                        new_code = self.llm.fix_code(new_code, tb)

                        # Ensure the class name persists in the corrected code
                        if isinstance(new_code, str) and new_class_name not in new_code:
                            new_code = new_code.replace(
                                parent_class.__name__, new_class_name, 1
                            )
                    else:
                        print("Max repair attempts reached. Discarding this mutation.")

            # Get parent's DB ID (not class name!)
            parent_db_id = getattr(parent_class, "_db_id", None) or self._strategy_db_ids.get(
                parent_class.__name__
            )

            if not success:
                # Register failed strategy in program DB
                if self.program_db:
                    from profit.program_db import StrategyStatus

                    failed_id = self.program_db.register_strategy(
                        code=new_code,
                        class_name=new_class_name,
                        parent_ids=[parent_db_id] if parent_db_id else [],
                        mutation_text=improvement,
                        metrics={},
                        tags=self._infer_tags(new_code),
                        status=StrategyStatus.COMPILE_FAILED,
                        generation=gen,
                        fold=fold,
                        diff_from_parent=diff_text or "",
                        val_return=None,
                        repair_attempts=repair_count,
                    )
                    self._strategy_db_ids[new_class_name] = failed_id
                    print(f"[{failed_id}] Failed: {new_class_name} ({repair_count} repair attempts)")
                continue  # Move to next generation

            # 14. Compute fitness of new strategy
            P_new = res["Return (Ann.) [%]"]
            print(
                f"New strategy variant '{new_class_name}' achieved validation "
                f"annual return {P_new:.2f}%"
            )

            # Prepare metrics for registration (extract all standard metrics)
            new_metrics = self._extract_standard_metrics(res)

            # 15-18. Check against MAS threshold
            if P_new is not None and P_new >= MAS:
                # Accept new strategy into population (with source code)
                population.append((NewStrategyClass, P_new, new_code))
                print(
                    f"Accepted new strategy (>= MAS={MAS:.2f}%). "
                    f"Population size now {len(population)}."
                )

                # Register accepted strategy in program DB
                if self.program_db:
                    from profit.program_db import StrategyStatus

                    child_id = self.program_db.register_strategy(
                        code=new_code,
                        class_name=new_class_name,
                        parent_ids=[parent_db_id] if parent_db_id else [],
                        mutation_text=improvement,
                        metrics=new_metrics,
                        tags=self._infer_tags(new_code),
                        status=StrategyStatus.ACCEPTED,
                        generation=gen,
                        fold=fold,
                        improvement_delta=P_new - parent_perf,
                        diff_from_parent=diff_text or "",
                        val_return=P_new,
                        repair_attempts=repair_count,
                    )
                    # CRITICAL: Track DB ID for accepted strategies
                    self._strategy_db_ids[new_class_name] = child_id
                    NewStrategyClass._db_id = child_id
                    repairs_str = f", {repair_count} repairs" if repair_count > 0 else ""
                    metrics_str = self._format_metrics_summary(new_metrics)
                    print(f"[{child_id}] Accepted: {new_class_name} ({metrics_str}{repairs_str})")

                # Persist the accepted strategy (existing persister)
                if self.persister and self.persister.run_dir:
                    metrics = {
                        "AnnReturn%": P_new,
                        "Sharpe": res.get("Sharpe Ratio"),
                        "Expectancy%": res.get("Expectancy [%]"),
                        "Trades": res.get("# Trades"),
                    }
                    self.persister.save_strategy(
                        strategy_class=NewStrategyClass,
                        source_code=new_code,
                        fold=fold,
                        generation=gen,
                        metrics=metrics,
                        parent_name=parent_class.__name__,
                        improvement_proposal=improvement,
                    )

                # Update best if this is highest so far
                if P_new > best_perf:
                    best_perf = P_new
                    best_strategy_class = NewStrategyClass
                    best_strategy_code = new_code
            else:
                # Register rejected strategy in program DB
                if self.program_db:
                    from profit.program_db import StrategyStatus

                    rejected_id = self.program_db.register_strategy(
                        code=new_code,
                        class_name=new_class_name,
                        parent_ids=[parent_db_id] if parent_db_id else [],
                        mutation_text=improvement,
                        metrics=new_metrics,
                        tags=self._infer_tags(new_code),
                        status=StrategyStatus.REJECTED,
                        generation=gen,
                        fold=fold,
                        improvement_delta=P_new - parent_perf,
                        diff_from_parent=diff_text or "",
                        val_return=P_new,
                        repair_attempts=repair_count,
                    )
                    self._strategy_db_ids[new_class_name] = rejected_id
                    repairs_str = f", {repair_count} repairs" if repair_count > 0 else ""
                    metrics_str = self._format_metrics_summary(new_metrics)
                    print(f"[{rejected_id}] Rejected: {new_class_name} ({metrics_str}{repairs_str}, below MAS={MAS:.2f}%)")
                else:
                    print(f"Discarded new strategy (did not meet MAS={MAS:.2f}%).")

        print(
            f"\nEvolution complete. Best strategy '{best_strategy_class.__name__}' "
            f"validation return = {best_perf:.2f}%."
        )
        return best_strategy_class, best_perf, best_strategy_code

    def walk_forward_optimize(
        self, full_data: pd.DataFrame, strategy_class, n_folds: int = 5
    ) -> list[dict]:
        """Perform walk-forward optimization across multiple folds.

        Evolves the strategy on each training/validation set, then evaluates
        on the test set. Compares performance against baseline strategies.

        Args:
            full_data: Complete historical DataFrame with datetime index and OHLCV columns.
            strategy_class: Seed strategy class to evolve.
            n_folds: Number of walk-forward folds (default: 5).

        Returns:
            List of per-fold result dictionaries containing:
            - fold: Fold number (1-indexed)
            - strategy: Best evolved strategy class
            - ann_return: Annualized return on test (%)
            - sharpe: Sharpe ratio on test
            - expectancy: Expectancy on test (%)
            - random_return: Random baseline return (%)
            - buy_hold_return: Buy-and-hold baseline return (%)
        """
        # Start persistence run
        if self.persister:
            run_dir = self.persister.start_run(
                seed_strategy_name=strategy_class.__name__,
                analyst_provider=self.llm.analyst_provider,
                analyst_model=self.llm.analyst_model,
                coder_provider=self.llm.coder_provider,
                coder_model=self.llm.coder_model,
            )
            print(f"Saving evolved strategies to: {run_dir}")

        folds = self.prepare_folds(full_data, n_folds=n_folds)
        results = []

        for i, (train, val, test) in enumerate(folds, start=1):
            print(f"\n=== Fold {i} ===")
            print(f"Training period: {train.index[0]} to {train.index[-1]}")
            print(f"Validation period: {val.index[0]} to {val.index[-1]}")
            print(f"Test period: {test.index[0]} to {test.index[-1]}")

            # Evolve strategy on this fold's data
            best_strat, _, best_code = self.evolve_strategy(strategy_class, train, val, fold=i)

            # Evaluate best strategy on test set
            metrics, res = self.run_backtest(best_strat, test)
            ann_return = metrics["AnnReturn%"]
            sharpe = metrics["Sharpe"]
            expectancy = metrics["Expectancy%"]
            print(
                f"Fold {i} Test Performance - Annualized Return: {ann_return:.2f}%, "
                f"Sharpe: {sharpe:.2f}, Expectancy: {expectancy:.2f}%"
            )

            # Update test metrics in program DB for overfitting detection
            if self.program_db:
                best_db_id = getattr(best_strat, "_db_id", None)
                if best_db_id:
                    self.program_db.update_test_metrics(best_db_id, ann_return)

            # Save best strategy for this fold
            if self.persister:
                self.persister.save_fold_best(i, best_strat, best_code, metrics)

            # Also evaluate baselines on the test set for comparison
            _, res_rand = self.run_backtest(RandomStrategy, test)
            _, res_bh = self.run_backtest(BuyAndHoldStrategy, test)
            rand_return = res_rand["Return (Ann.) [%]"]
            bh_return = res_bh["Return (Ann.) [%]"]
            print(
                f"Fold {i} Baselines - Random Strat Return: {rand_return:.2f}%, "
                f"Buy&Hold Return: {bh_return:.2f}%"
            )

            results.append({
                "fold": i,
                "strategy": best_strat,
                "ann_return": ann_return,
                "sharpe": sharpe,
                "expectancy": expectancy,
                "random_return": rand_return,
                "buy_hold_return": bh_return,
            })

        # Summarize across folds
        avg_ret = np.mean([r["ann_return"] for r in results])
        avg_bh = np.mean([r["buy_hold_return"] for r in results])
        avg_rand = np.mean([r["random_return"] for r in results])

        print(f"\nAverage Annualized Return over {len(results)} folds: {avg_ret:.2f}%")
        print(f"Average Buy-and-Hold Return over {len(results)} folds: {avg_bh:.2f}%")
        print(f"Average Random Strategy Return over {len(results)} folds: {avg_rand:.2f}%")

        # Finalize persistence run
        if self.persister:
            summary_path = self.persister.finalize_run(results)
            print(f"\nRun summary saved to: {summary_path}")

        # Print program DB summary
        self._print_strategy_summary()

        return results

    def _build_strategy_name(self, parent_name: str, generation: int) -> str:
        """Build strategy name with generation chain.

        Format: {BaseName}_{CurrentGen}_{ParentGen1}_{ParentGen2}...
        Example: EMACrossover_15_7_13 (gen 15 evolved from gen 13 evolved from gen 7)

        Args:
            parent_name: Name of the parent strategy class.
            generation: Current generation number.

        Returns:
            New strategy class name with generation chain.
        """
        # Extract base name (original strategy without generation suffixes)
        parts = parent_name.split("_")
        base = parts[0]  # e.g., "EMACrossover"

        if len(parts) > 1:
            # Parent has generation chain, prepend current gen
            gen_chain = "_".join(parts[1:])  # e.g., "7" or "13_7"
            return f"{base}_{generation}_{gen_chain}"
        else:
            # Parent is seed, just use current gen
            return f"{base}_{generation}"

    def _infer_tags(self, code: str) -> list[str]:
        """Infer strategy tags from code content.

        Args:
            code: Strategy source code.

        Returns:
            List of inferred tags.
        """
        tags = []
        code_lower = code.lower()

        if "bollinger" in code_lower or "mean" in code_lower:
            tags.append("mean-reversion")
        if "ema" in code_lower or "sma" in code_lower or "crossover" in code_lower:
            tags.append("trend")
        if "rsi" in code_lower or "cci" in code_lower or "williams" in code_lower:
            tags.append("oscillator")
        if "macd" in code_lower:
            tags.append("momentum")
        if "atr" in code_lower or "stop" in code_lower:
            tags.append("risk-management")

        return tags if tags else ["unclassified"]

    def _print_strategy_summary(self) -> None:
        """Print a summary table of all strategies registered during this run."""
        if not self.program_db or not self._strategy_db_ids:
            return

        print("\n" + "=" * 120)
        print("STRATEGY DATABASE SUMMARY")
        print("=" * 120)
        print(
            f"{'ID':<10} {'Class Name':<24} {'Gen':>3} {'Fix':>3} "
            f"{'Status':<8} {'Return':>8} {'Sharpe':>7} {'MaxDD':>7} "
            f"{'Trades':>6} {'Win%':>5} {'Test':>8}"
        )
        print("-" * 120)

        # Count by status
        accepted_count = 0
        rejected_count = 0
        failed_count = 0
        seed_count = 0

        # Get all strategies from this run (by tracking IDs we registered)
        for class_name, db_id in sorted(
            self._strategy_db_ids.items(), key=lambda x: x[0]
        ):
            record = self.program_db.get_strategy(db_id)
            if record:
                metrics = record.metrics

                # Validation return (prefer val_return, fall back to metrics)
                val_ret = record.val_return
                if val_ret is None:
                    val_ret = metrics.get("ann_return")
                val_str = f"{val_ret:.1f}%" if val_ret is not None else "N/A"

                # Sharpe ratio
                sharpe = metrics.get("sharpe")
                sharpe_str = f"{sharpe:.2f}" if sharpe is not None else "-"

                # Max drawdown
                max_dd = metrics.get("max_drawdown")
                dd_str = f"{max_dd:.1f}%" if max_dd is not None else "-"

                # Trade count
                trades = metrics.get("trade_count")
                trades_str = f"{int(trades)}" if trades is not None else "-"

                # Win rate
                win_rate = metrics.get("win_rate")
                win_str = f"{win_rate:.0f}%" if win_rate is not None else "-"

                # Test return (filled after fold completion)
                test_str = f"{record.test_return:.1f}%" if record.test_return is not None else "-"

                # Repair attempts
                repairs_str = str(record.repair_attempts) if record.repair_attempts > 0 else "0"

                status_str = record.status.value[:8]  # Truncate status
                class_display = record.class_name[:24]  # Truncate class name

                print(
                    f"[{db_id}] {class_display:<24} {record.generation:>3} "
                    f"{repairs_str:>3} {status_str:<8} {val_str:>8} {sharpe_str:>7} "
                    f"{dd_str:>7} {trades_str:>6} {win_str:>5} {test_str:>8}"
                )
                # Count by status
                if record.status.value == "accepted":
                    accepted_count += 1
                elif record.status.value == "rejected":
                    rejected_count += 1
                elif record.status.value == "compile_failed":
                    failed_count += 1
                elif record.status.value == "seed":
                    seed_count += 1

        print("-" * 120)
        total = len(self._strategy_db_ids)
        print(
            f"Total: {total} | Seeds: {seed_count} | Accepted: {accepted_count} | "
            f"Rejected: {rejected_count} | Failed: {failed_count}"
        )
        print("=" * 120)

    def _format_metrics_summary(self, metrics: Dict[str, float]) -> str:
        """Format metrics dict into a readable summary string.

        Args:
            metrics: Dict of metric name to value.

        Returns:
            Formatted string like "return=15.2%, sharpe=1.2, dd=-8.5%, trades=42"
        """
        parts = []

        # Return (always show)
        if "ann_return" in metrics:
            parts.append(f"return={metrics['ann_return']:.1f}%")

        # Sharpe (key risk-adjusted metric)
        if "sharpe" in metrics:
            parts.append(f"sharpe={metrics['sharpe']:.2f}")

        # Max drawdown (key risk metric)
        if "max_drawdown" in metrics:
            parts.append(f"dd={metrics['max_drawdown']:.1f}%")

        # Trade count
        if "trade_count" in metrics:
            parts.append(f"trades={int(metrics['trade_count'])}")

        # Win rate (if available)
        if "win_rate" in metrics:
            parts.append(f"win={metrics['win_rate']:.0f}%")

        # Sortino (if available and different insight from sharpe)
        if "sortino" in metrics:
            parts.append(f"sortino={metrics['sortino']:.2f}")

        return ", ".join(parts)

    def _extract_standard_metrics(self, result) -> Dict[str, float]:
        """Extract all standard metrics from backtest result.

        Maps backtesting.py result keys to STANDARD_METRICS schema defined
        in program_db.py for consistent metric storage.

        Args:
            result: Backtesting.py result Series.

        Returns:
            Dict mapping standard metric names to values.
        """
        # Handle duration conversion for avg_holding_period
        avg_duration = result.get("Avg. Trade Duration")
        if avg_duration is not None and hasattr(avg_duration, "total_seconds"):
            # Convert timedelta to days
            avg_holding_days = avg_duration.total_seconds() / 86400
        else:
            avg_holding_days = None

        metrics = {
            # Returns
            "ann_return": result.get("Return (Ann.) [%]"),
            "total_return": result.get("Return [%]"),
            # Risk-adjusted
            "sharpe": result.get("Sharpe Ratio"),
            "sortino": result.get("Sortino Ratio"),
            "calmar": result.get("Calmar Ratio"),
            # Risk
            "max_drawdown": result.get("Max. Drawdown [%]"),
            "volatility": result.get("Volatility (Ann.) [%]"),
            # Trading behavior
            "trade_count": result.get("# Trades"),
            "win_rate": result.get("Win Rate [%]"),
            "avg_trade_return": result.get("Avg. Trade [%]"),
            "avg_holding_period": avg_holding_days,
            "exposure_time": result.get("Exposure Time [%]"),
            # Profit metrics
            "profit_factor": result.get("Profit Factor"),
            "expectancy": result.get("Expectancy [%]"),
        }

        # Filter out None and NaN values
        return {
            k: v for k, v in metrics.items()
            if v is not None and not (isinstance(v, float) and np.isnan(v))
        }

    def _should_use_diffs(self, generation: int) -> bool:
        """Determine whether to use diff-based mutations for this generation.

        Phase 14: Implements adaptive diff/rewrite selection based on:
        - diff_mode setting ("always", "never", "adaptive")
        - exploration_gens threshold
        - consecutive diff failure tracking

        Args:
            generation: Current evolution generation (1-indexed).

        Returns:
            True if diffs should be attempted, False for full rewrite.
        """
        if not self.prefer_diffs:
            return False

        if self.diff_mode == "never":
            return False

        if self.diff_mode == "always":
            return True

        # Adaptive mode
        # Early generations: explore with full rewrites
        if generation <= self.exploration_gens:
            return False

        # If diffs are failing repeatedly, switch to rewrites
        if self._consecutive_diff_failures >= 2:
            return False

        return True
